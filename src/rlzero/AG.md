# 穿越时空的“内心戏”

> 2025-03-06

![AG](./img/AG.jpg)

## <<收获与播种>>

- 起手先感谢一手DeepSeekR1，能够提供法文翻译，让我读到这么好的作品。

### <<收获与播种>> 读了100多页，发现作者在试图勾勒自己内心的“天真”。
#### ——A·G回忆在 17岁时候自己的探索“面积、体积观”的内观，和算术几何建立遵循一样的内在的天真。
#### 这启发了我“自己引导自己构建”与RL的完整对应：
- 正反馈只有
- 1. 格式奖励：
  - 1.1 \<think\> -> \<Answer\> 形成了箭头
- 2. 结果奖励:
  - 2.1 “直观的侧面、外化与心”（不是什么），指向结果奖励
  - 2.2 验证器，只要有GroundTruth，就会有奖励
  - 2.3 确认了箭头


> 概型扇开的有向图 + 某种连通性(拓普斯)的对应 <-> “RL训练时候与自己的策略函数里的不太远” 两者是等价的


- 工匠 对应 利用 exploit
- 探索者 对应 探索 explore
- 诚实 对应 真诚的接收feedback
- 孤独 对应 （他之前描绘的别的数学家在框定内工作，用别人的约束和反馈），自己则是孤独地自己指导、约束和反馈给自己。


——————

> 2025-11-10

## Data-driven Approximate Dynamic Programming

[什么是动态规划（Dynamic Programming）？动态规划的意义是什么？ - 覃含章的回答 - 知乎](https://www.zhihu.com/question/23995189/answer/723475721)

### 事实上，任何一个有限状态空间的MDP(马尔科夫决策)问题都可以写成最短路的问题，反之亦然。

使用DP(动态规划)求解，但其本身却只蕴含一个和暴力枚举差不多的基本算法，尤其在决策空间维度很大时，DP算法会遭受著名的维数诅咒，即算法的求解时间随问题规模指数级增长。 因此，为了真正求解复杂的动态规划，我们实际上只能求近似解(ADP所谓近似动态规划)。 这几年，伴随着机器学习的热潮，【利用数据驱动的ADP类算法】也渐渐再度被人们熟悉，尤其是其中的一些近似算法(如RL)。

研究难点就在于MDP作为泛用的建模工具，针对高维问题的求解算法设计及求解方法。

（——回看PUCT中，通过蒙特卡洛近似监督出策略网络 等价于 模拟出一个策略梯度上升算法）


### 抽象动态规划的框架在马尔科夫决策问题上，贝尔曼更新能够成为压缩映射。

在(统计推动或)数据驱动的RL算法，期望操作确保价值信息在状态之间"扩散"，而不是集中在某个特定路径上。同时期望也利用了概率的凸组合性质（期望平滑了差异）——按概率加权平均将不同状态下的差异|V(s')-U(s')|进行了平滑，使得整体差异被bound住，从而避免了极端情况下差异的放大，加速收敛，没有这个平滑效应，即使有折扣因子 γ，也可能无法保证收敛（在 γ 接近1时尤其重要）。这个深刻的数学性质解释了为什么基于数据驱动的近似动态规划的强化学习算法在复杂的随机环境中依然能够稳定收敛。
